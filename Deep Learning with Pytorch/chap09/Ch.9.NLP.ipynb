{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31f52f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Seungyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Is', 'it', 'possible', 'distinguishing', 'cats', 'and', 'dogs']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "text = nltk.word_tokenize('Is it possible distinguishing cats and dogs')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ecd62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Seungyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f89a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Is', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('possible', 'JJ'),\n",
       " ('distinguishing', 'VBG'),\n",
       " ('cats', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('dogs', 'NNS')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de0d9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Seungyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "string1 = 'my favorite subject is math'\n",
    "string2 = 'my favorite subject is math, english, economic and computer science'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3064343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'favorite', 'subject', 'is', 'math']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10f8986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my',\n",
       " 'favorite',\n",
       " 'subject',\n",
       " 'is',\n",
       " 'math',\n",
       " ',',\n",
       " 'english',\n",
       " ',',\n",
       " 'economic',\n",
       " 'and',\n",
       " 'computer',\n",
       " 'science']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c06dcd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from konlpy.tag import Komoran\n",
    "# komoran = Komoran()\n",
    "# print(komoran.morphs('딥러닝이 쉽나요? 어렵나요?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e87a3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(komoran.pos('소파 위에 있는 것이 고양이인가요? 강아지인가요?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805cb62f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafcf4a3",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "163a1aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tissue</th>\n",
       "      <th>class</th>\n",
       "      <th>class2</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>mdb000</td>\n",
       "      <td>C</td>\n",
       "      <td>CIRC</td>\n",
       "      <td>N</td>\n",
       "      <td>535.0</td>\n",
       "      <td>475.0</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mdb001</td>\n",
       "      <td>A</td>\n",
       "      <td>CIRA</td>\n",
       "      <td>N</td>\n",
       "      <td>433.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>mdb002</td>\n",
       "      <td>A</td>\n",
       "      <td>CIRA</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>mdb003</td>\n",
       "      <td>C</td>\n",
       "      <td>CIRC</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mdb004</td>\n",
       "      <td>F</td>\n",
       "      <td>CIRF</td>\n",
       "      <td>I</td>\n",
       "      <td>488.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>mdb005</td>\n",
       "      <td>F</td>\n",
       "      <td>CIRF</td>\n",
       "      <td>B</td>\n",
       "      <td>544.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      id tissue class class2      x      y      r\n",
       "0           0  mdb000      C  CIRC      N  535.0  475.0  192.0\n",
       "1           1  mdb001      A  CIRA      N  433.0  268.0   58.0\n",
       "2           2  mdb002      A  CIRA      I    NaN    NaN    NaN\n",
       "3           3  mdb003      C  CIRC      B    NaN    NaN    NaN\n",
       "4           4  mdb004      F  CIRF      I  488.0  145.0   29.0\n",
       "5           5  mdb005      F  CIRF      B  544.0  178.0   26.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../chap09/data/class2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f654aac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "id            0\n",
       "tissue        0\n",
       "class         0\n",
       "class2        0\n",
       "x             2\n",
       "y             2\n",
       "r             2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece183af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0.000000\n",
       "id            0.000000\n",
       "tissue        0.000000\n",
       "class         0.000000\n",
       "class2        0.000000\n",
       "x             0.333333\n",
       "y             0.333333\n",
       "r             0.333333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bd7a342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0      id tissue class class2      x      y      r\n",
      "0           0  mdb000      C  CIRC      N  535.0  475.0  192.0\n",
      "1           1  mdb001      A  CIRA      N  433.0  268.0   58.0\n",
      "2           2  mdb002      A  CIRA      I    NaN    NaN    NaN\n",
      "3           3  mdb003      C  CIRC      B    NaN    NaN    NaN\n",
      "4           4  mdb004      F  CIRF      I  488.0  145.0   29.0\n",
      "5           5  mdb005      F  CIRF      B  544.0  178.0   26.0\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(how='all')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43de10fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0      id tissue class class2      x      y      r\n",
      "0           0  mdb000      C  CIRC      N  535.0  475.0  192.0\n",
      "1           1  mdb001      A  CIRA      N  433.0  268.0   58.0\n",
      "4           4  mdb004      F  CIRF      I  488.0  145.0   29.0\n",
      "5           5  mdb005      F  CIRF      B  544.0  178.0   26.0\n"
     ]
    }
   ],
   "source": [
    "df1 = df.dropna()\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8017495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tissue</th>\n",
       "      <th>class</th>\n",
       "      <th>class2</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>mdb000</td>\n",
       "      <td>C</td>\n",
       "      <td>CIRC</td>\n",
       "      <td>N</td>\n",
       "      <td>535.0</td>\n",
       "      <td>475.0</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mdb001</td>\n",
       "      <td>A</td>\n",
       "      <td>CIRA</td>\n",
       "      <td>N</td>\n",
       "      <td>433.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>mdb002</td>\n",
       "      <td>A</td>\n",
       "      <td>CIRA</td>\n",
       "      <td>I</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>mdb003</td>\n",
       "      <td>C</td>\n",
       "      <td>CIRC</td>\n",
       "      <td>B</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mdb004</td>\n",
       "      <td>F</td>\n",
       "      <td>CIRF</td>\n",
       "      <td>I</td>\n",
       "      <td>488.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>mdb005</td>\n",
       "      <td>F</td>\n",
       "      <td>CIRF</td>\n",
       "      <td>B</td>\n",
       "      <td>544.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      id tissue class class2      x      y      r\n",
       "0           0  mdb000      C  CIRC      N  535.0  475.0  192.0\n",
       "1           1  mdb001      A  CIRA      N  433.0  268.0   58.0\n",
       "2           2  mdb002      A  CIRA      I    0.0    0.0    0.0\n",
       "3           3  mdb003      C  CIRC      B    0.0    0.0    0.0\n",
       "4           4  mdb004      F  CIRF      I  488.0  145.0   29.0\n",
       "5           5  mdb005      F  CIRF      B  544.0  178.0   26.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.fillna(0)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "968a2c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tissue</th>\n",
       "      <th>class</th>\n",
       "      <th>class2</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>mdb000</td>\n",
       "      <td>C</td>\n",
       "      <td>CIRC</td>\n",
       "      <td>N</td>\n",
       "      <td>535.0</td>\n",
       "      <td>475.0</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mdb001</td>\n",
       "      <td>A</td>\n",
       "      <td>CIRA</td>\n",
       "      <td>N</td>\n",
       "      <td>433.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>mdb002</td>\n",
       "      <td>A</td>\n",
       "      <td>CIRA</td>\n",
       "      <td>I</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>mdb003</td>\n",
       "      <td>C</td>\n",
       "      <td>CIRC</td>\n",
       "      <td>B</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mdb004</td>\n",
       "      <td>F</td>\n",
       "      <td>CIRF</td>\n",
       "      <td>I</td>\n",
       "      <td>488.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>mdb005</td>\n",
       "      <td>F</td>\n",
       "      <td>CIRF</td>\n",
       "      <td>B</td>\n",
       "      <td>544.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      id tissue class class2      x      y      r\n",
       "0           0  mdb000      C  CIRC      N  535.0  475.0  192.0\n",
       "1           1  mdb001      A  CIRA      N  433.0  268.0   58.0\n",
       "2           2  mdb002      A  CIRA      I  500.0    NaN    NaN\n",
       "3           3  mdb003      C  CIRC      B  500.0    NaN    NaN\n",
       "4           4  mdb004      F  CIRF      I  488.0  145.0   29.0\n",
       "5           5  mdb005      F  CIRF      B  544.0  178.0   26.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['x'].fillna(df['x'].mean(), inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c11152",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bee76c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing, or NLP, is the process of extracting the meaning, or intent, behind human language.',\n",
       " 'In the field of Conversational artificial intelligence (AI), NLP allows machines and applications to understand the intent of human language inputs, and then generate appropriate responses, resulting in a natural conversation flow.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "text_sample = 'Natural Language Processing, or NLP, is the process of extracting the meaning, or intent, behind human language. In the field of Conversational artificial intelligence (AI), NLP allows machines and applications to understand the intent of human language inputs, and then generate appropriate responses, resulting in a natural conversation flow.'\n",
    "tokenized_sentences = sent_tokenize(text_sample)\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ab63f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'book', 'is', 'for', 'deep', 'learning', 'learners']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "sentence = \"This book is for deep learning learners\"\n",
    "words = word_tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3ffe429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'nothing',\n",
       " 'that',\n",
       " 'you',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'already',\n",
       " 'know',\n",
       " 'except',\n",
       " 'most',\n",
       " 'people',\n",
       " 'aren',\n",
       " \"'\",\n",
       " 't',\n",
       " 'aware',\n",
       " 'of',\n",
       " 'how',\n",
       " 'their',\n",
       " 'inner',\n",
       " 'world',\n",
       " 'works',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "sentence = \"it's nothing that you don't already know except most people aren't aware of how their inner world works.\"\n",
    "words = WordPunctTokenizer().tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efafac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# from konlpy.tag import Okt\n",
    "# from gensim.models import word2vec\n",
    "\n",
    "# f = open(r'../chap09/data/ratings_train.txt', 'r', encoding='utf-8')\n",
    "# rdr = csv.reader(f, delimiter='\\t')\n",
    "# rdw = list(rdr)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31516cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter = Okt()\n",
    "\n",
    "# result = []\n",
    "# for line in rdw:\n",
    "#     malist = twitter.pos( line[1], norm=True, stem=True)\n",
    "#     r = []\n",
    "#     for word in malist:\n",
    "#         if not word[1] in [\"Josa\",\"Eomi\",\"Punctuation\"]:\n",
    "#             r.append(word[0])\n",
    "#     rl = (\" \".join(r)).strip()\n",
    "#     result.append(rl)\n",
    "#     print(rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a31b8813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"NaverMovie.nlp\",'w', encoding='utf-8') as fp:\n",
    "#     fp.write(\"\\n\".join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f8e7d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mData = word2vec.LineSentence(\"NaverMovie.nlp\")\n",
    "# mModel =word2vec.Word2Vec(mData, vector_size=200, window=10, hs=1, min_count=2, sg=1)\n",
    "# mModel.save(\"NaverMovie.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd1416",
   "metadata": {},
   "source": [
    "### Stop word eliminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fb7c76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 미적용: ['One', 'of', 'the', 'first', 'things', 'that', 'we', 'ask', 'ourselves', 'is', 'what', 'are', 'the', 'pros', 'and', 'cons', 'of', 'any', 'task', 'we', 'perform'] \n",
      "\n",
      "불용어 제거 적용: ['One', 'first', 'things', 'ask', 'pros', 'cons', 'task', 'perform']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Seungyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Seungyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sample_text = \"One of the first things that we ask ourselves is what are the pros and cons of any task we perform\"\n",
    "text_tokens = word_tokenize(sample_text)\n",
    "\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
    "\n",
    "print(\"불용어 제거 미적용:\", text_tokens, '\\n')\n",
    "print(\"불용어 제거 적용:\", tokens_without_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1e35a",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d4c35f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obess obsses\n",
      "standard standard\n",
      "nation nation\n",
      "absent absent\n",
      "tribal tribalic\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem('obesses'), stemmer.stem('obssesed'))\n",
    "print(stemmer.stem('standardizes'), stemmer.stem('standardization'))\n",
    "print(stemmer.stem('national'), stemmer.stem('nation'))\n",
    "print(stemmer.stem('absentness'), stemmer.stem('absently'))\n",
    "print(stemmer.stem('tribalical'), stemmer.stem('tribalicalized'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ab32902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obess obsses\n",
      "standard standard\n",
      "nat nat\n",
      "abs abs\n",
      "trib trib\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('obesses'), stemmer.stem('obssesed'))\n",
    "print(stemmer.stem('standardizes'), stemmer.stem('standardization'))\n",
    "print(stemmer.stem('national'), stemmer.stem('nation'))\n",
    "print(stemmer.stem('absentness'), stemmer.stem('absently'))\n",
    "print(stemmer.stem('tribalical'), stemmer.stem('tribalicalized'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4387f36b",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d34f36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Seungyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obesses obssesed\n",
      "standardizes standardization\n",
      "national nation\n",
      "absentness absently\n",
      "tribalical tribalicalized\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "print(lemma.lemmatize('obesses'), lemma.lemmatize('obssesed'))\n",
    "print(lemma.lemmatize('standardizes'), lemma.lemmatize('standardization'))\n",
    "print(lemma.lemmatize('national'), lemma.lemmatize('nation'))\n",
    "print(lemma.lemmatize('absentness'), lemma.lemmatize('absently'))\n",
    "print(lemma.lemmatize('tribalical'), lemma.lemmatize('tribalicalized'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d212d7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obesses obssesed\n",
      "standardize standardization\n",
      "national nation\n",
      "absentness absently\n",
      "tribalical tribalicalized\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize('obesses', 'v'), lemma.lemmatize('obssesed','a'))\n",
    "print(lemma.lemmatize('standardizes','v'), lemma.lemmatize('standardization','n'))\n",
    "print(lemma.lemmatize('national','a'), lemma.lemmatize('nation','n'))\n",
    "print(lemma.lemmatize('absentness','n'), lemma.lemmatize('absently','r'))\n",
    "print(lemma.lemmatize('tribalical','a'), lemma.lemmatize('tribalicalized','v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c255776",
   "metadata": {},
   "source": [
    "### normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22a16a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ac10ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../chap09/data/diabetes.csv')\n",
    "X = df[df.columns[:-1]]\n",
    "y = df['Outcome']\n",
    "\n",
    "X = X.values\n",
    "y = torch.tensor(y.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15088207",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = MinMaxScaler()\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.fit_transform(X_test)\n",
    "y_train =y_train.reshape(-1, 1)\n",
    "y_test =y_test.reshape(-1, 1)\n",
    "y_train = ms.fit_transform(y_train)\n",
    "y_test = ms.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6989a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customdataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = len(self.X)\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90dcd9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = customdataset(torch.FloatTensor(X_train), \n",
    "                       torch.FloatTensor(y_train))\n",
    "test_data = customdataset(torch.FloatTensor(X_test), \n",
    "                       torch.FloatTensor(y_test))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f14cb1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        self.layer_1 = nn.Linear(8, 64, bias=True) \n",
    "        self.layer_2 = nn.Linear(64, 64, bias=True)\n",
    "        self.layer_out = nn.Linear(64, 1, bias=True)         \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2179fa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binaryClassification(\n",
      "  (layer_1): Linear(in_features=8, out_features=64, bias=True)\n",
      "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000+1\n",
    "print_epoch = 100\n",
    "lr = 1e-2\n",
    "\n",
    "model = binaryClassification()\n",
    "model.to(device)\n",
    "print(model)\n",
    "BCE = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1af86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc*100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb96bca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: epoch: 0 - loss:0.70710; acc: 51.778\n",
      "Test: epoch: 0 - loss:0.68129; acc: 180.750\n",
      "Train: epoch: 100 - loss:0.38244; acc: 83.667\n",
      "Test: epoch: 100 - loss:0.48891; acc: 265.500\n",
      "Train: epoch: 200 - loss:0.41354; acc: 80.556\n",
      "Test: epoch: 200 - loss:0.48894; acc: 258.750\n",
      "Train: epoch: 300 - loss:0.42106; acc: 77.111\n",
      "Test: epoch: 300 - loss:0.49784; acc: 249.000\n",
      "Train: epoch: 400 - loss:0.38211; acc: 83.222\n",
      "Test: epoch: 400 - loss:0.49850; acc: 264.250\n",
      "Train: epoch: 500 - loss:0.42276; acc: 78.222\n",
      "Test: epoch: 500 - loss:0.58917; acc: 253.500\n",
      "Train: epoch: 600 - loss:0.51422; acc: 79.111\n",
      "Test: epoch: 600 - loss:0.49473; acc: 252.750\n",
      "Train: epoch: 700 - loss:0.34827; acc: 83.556\n",
      "Test: epoch: 700 - loss:0.50145; acc: 264.250\n",
      "Train: epoch: 800 - loss:0.36610; acc: 82.889\n",
      "Test: epoch: 800 - loss:0.51372; acc: 263.250\n",
      "Train: epoch: 900 - loss:0.30763; acc: 85.889\n",
      "Test: epoch: 900 - loss:0.54667; acc: 270.250\n",
      "Train: epoch: 1000 - loss:0.38958; acc: 78.444\n",
      "Test: epoch: 1000 - loss:0.52231; acc: 252.750\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    iteration_loss = 0.\n",
    "    iteration_accuracy = 0.\n",
    "    \n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        X, y = data\n",
    "        X, y= X.to(device), y.to(device)\n",
    "        y_pred = model(X.float())\n",
    "        loss= BCE(y_pred, y.reshape(-1,1).float())\n",
    "        \n",
    "        iteration_loss += loss\n",
    "        iteration_accuracy += accuracy(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch %print_epoch == 0):\n",
    "        print('Train: epoch: {0} - loss:{1:.5f}; acc: {2:.3f}'.format(epoch, iteration_loss/(i+1), iteration_accuracy/(i+1)))\n",
    "        \n",
    "    iteration_loss = 0.\n",
    "    itreation_accuracy = 0.\n",
    "    model.eval()\n",
    "    \n",
    "    for i, data in enumerate(test_loader):\n",
    "        X,y = data\n",
    "        X, y= X.to(device), y.to(device)\n",
    "        y_pred = model(X.float())\n",
    "        loss = BCE(y_pred, y.reshape(-1,1).float())\n",
    "        iteration_loss += loss\n",
    "        iteration_accuracy += accuracy(y_pred,y)\n",
    "        \n",
    "    if (epoch %print_epoch == 0):\n",
    "        print('Test: epoch: {0} - loss:{1:.5f}; acc: {2:.3f}'.format(epoch, iteration_loss/(i+1), iteration_accuracy/(i+1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_book",
   "language": "python",
   "name": "torch_book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
